\begin{tabular}{l p{37em}}
\toprule
          Method &                                                                                                                                                          Hyperparameters \\
\midrule
        AdaBoost &                                                                                               \{'learning\_rate': (0.01, 0.1, 1.0, 10.0), 'n\_estimators': (10, 100, 1000)\} \\
     KernelRidge &                                                           \{'kernel': ('linear', 'poly', 'rbf', 'sigmoid'), 'alpha': (0.0001, 0.01, 0.1, 1), 'gamma': (0.01, 0.1, 1, 10)\} \\
       LassoLars &                                                                                                                                 \{'alpha': (0.0001, 0.001, 0.01, 0.1, 1)\} \\
            LGBM & \{'n\_estimators': (10, 50, 100, 250, 500, 1000), 'learning\_rate': (0.0001, 0.01, 0.05, 0.1, 0.2), 'subsample': (0.5, 0.75, 1), 'boosting\_type': ('gbdt', 'dart', 'goss')\} \\
LinearRegression &                                                                                                                                               \{'fit\_intercept': (True,)\} \\
             MLP &                                \{'activation': ('logistic', 'tanh', 'relu'), 'solver': ('lbfgs', 'adam', 'sgd'), 'learning\_rate': ('constant', 'invscaling', 'adaptive')\} \\
    RandomForest &                                                  \{'n\_estimators': (10, 100, 1000), 'min\_weight\_fraction\_leaf': (0.0, 0.25, 0.5), 'max\_features': ('sqrt', 'log2', None)\} \\
             SGD &                                                                                               \{'alpha': (1e-06, 0.0001, 0.01, 1), 'penalty': ('l2', 'l1', 'elasticnet')\} \\
             XGB &          \{'n\_estimators': (10, 50, 100, 250, 500, 1000), 'learning\_rate': (0.0001, 0.01, 0.05, 0.1, 0.2), 'gamma': (0, 0.1, 0.2, 0.3, 0.4), 'subsample': (0.5, 0.75, 1)\} \\
\bottomrule
\end{tabular}
