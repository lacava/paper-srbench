
@article{abolafiaNeuralProgramSynthesis2018,
  title = {Neural Program Synthesis with Priority Queue Training},
  author = {Abolafia, Daniel A. and Norouzi, Mohammad and Shen, Jonathan and Zhao, Rui and Le, Quoc V.},
  year = {2018},
  archiveprefix = {arXiv},
  eprint = {1801.03526},
  eprinttype = {arxiv},
  file = {/media/bill/Drive/zotero/data/storage/IG6XIAXA/Abolafia et al. - 2018 - Neural program synthesis with priority queue train.pdf;/media/bill/Drive/zotero/data/storage/BM76I99D/1801.html},
  journal = {arXiv preprint arXiv:1801.03526}
}

@inproceedings{arnaldoBuildingPredictiveModels2015,
  title = {Building {{Predictive Models}} via {{Feature Synthesis}}},
  booktitle = {Conference on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Arnaldo, Ignacio and O'Reilly, Una-May and Veeramachaneni, Kalyan},
  year = {2015},
  pages = {983--990},
  publisher = {{ACM Press}},
  doi = {10.1145/2739480.2754693},
  file = {/media/bill/Drive/zotero/data/storage/BZJ7K6DM/p983.pdf},
  isbn = {978-1-4503-3472-3},
  language = {en}
}

@inproceedings{arnaldoMultipleRegressionGenetic2014a,
  title = {Multiple Regression Genetic Programming},
  booktitle = {Proceedings of the 2014 {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Arnaldo, Ignacio and Krawiec, Krzysztof and O'Reilly, Una-May},
  year = {2014},
  pages = {879--886},
  publisher = {{ACM}}
}

@article{azadKrzysztofKrawiecBehavioral2017,
  title = {Krzysztof {{Krawiec}}: {{Behavioral}} Program Synthesis with Genetic Programming},
  shorttitle = {Krzysztof {{Krawiec}}},
  author = {Azad, Raja Muhammad Atif},
  year = {2017},
  month = mar,
  volume = {18},
  pages = {111--113},
  issn = {1389-2576, 1573-7632},
  doi = {10.1007/s10710-016-9283-7},
  file = {/media/bill/Drive/zotero/data/storage/4PGH272A/Azad - 2017 - Krzysztof Krawiec Behavioral program synthesis wi.pdf;/media/bill/Drive/zotero/data/storage/R9842R7M/Azad - 2017 - Krzysztof Krawiec Behavioral program synthesis wi.html},
  journal = {Genetic Programming and Evolvable Machines},
  language = {en},
  number = {1}
}

@article{bongardNonlinearSystemIdentification2005a,
  title = {Nonlinear {{System Identification Using Coevolution}} of {{Models}} and {{Tests}}},
  author = {Bongard, J.C. and Lipson, H.},
  year = {2005},
  month = aug,
  volume = {9},
  pages = {361--384},
  issn = {1089-778X},
  doi = {10.1109/TEVC.2005.850293},
  abstract = {We present a coevolutionary algorithm for inferring the topology and parameters of a wide range of hidden nonlinear systems with a minimum of experimentation on the target system. The algorithm synthesizes an explicit model directly from the observed data produced by intelligently generated tests. The algorithm is composed of two coevolving populations. One population evolves candidate models that estimate the structure of the hidden system. The second population evolves informative tests that either extract new information from the hidden system or elicit desirable behavior from it. The fitness of candidate models is their ability to explain behavior of the target system observed in response to all tests carried out so far; the fitness of candidate tests is their ability to make the models disagree in their predictions. We demonstrate the generality of this estimation-exploration algorithm by applying it to four different problems\textemdash grammar induction, gene network inference, evolutionary robotics, and robot damage recovery\textemdash and discuss how it overcomes several of the pathologies commonly found in other coevolutionary algorithms. We show that the algorithm is able to successfully infer and/or manipulate highly nonlinear hidden systems using very few tests, and that the benefit of this approach increases as the hidden systems possess more degrees of freedom, or become more biased or unobservable. The algorithm provides a systematic method for posing synthesis or analysis tasks to a coevolutionary system.},
  journal = {IEEE Transactions on Evolutionary Computation},
  keywords = {candidate tests,Coevolution,Data mining,estimation-exploration algorithm,Evolutionary computation,evolutionary robotics,gene network inference,grammar induction,identification,Inference algorithms,inference mechanisms,intelligent generated tests,Iterative methods,model coevolutionary computation,nonlinear hidden system,Nonlinear System Identification,nonlinear systems,nonlinear topological system identification,Pathology,Predictive models,robot damage recovery,robots,system identification,System testing,Topology},
  number = {4}
}

@article{breimanRandomForests2001b,
  title = {Random Forests},
  author = {Breiman, Leo},
  year = {2001},
  volume = {45},
  pages = {5--32},
  publisher = {{Springer}},
  journal = {Machine learning},
  number = {1}
}

@inproceedings{burlacuOperonEfficientGenetic2020,
  title = {Operon {{C}}++ an Efficient Genetic Programming Framework for Symbolic Regression},
  booktitle = {Proceedings of the 2020 {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Burlacu, Bogdan and Kronberger, Gabriel and Kommenda, Michael},
  year = {2020},
  pages = {1562--1570},
  file = {/media/bill/Drive/zotero/data/storage/M6MF7DFV/Burlacu et al. - 2020 - Operon C++ an efficient genetic programming framew.pdf;/media/bill/Drive/zotero/data/storage/779XNQP2/3377929.html}
}

@article{castelliFrameworkGeometricSemantic2015,
  title = {A {{C}}++ Framework for Geometric Semantic Genetic Programming},
  author = {Castelli, Mauro and Silva, Sara and Vanneschi, Leonardo},
  year = {2015},
  month = mar,
  volume = {16},
  pages = {73--81},
  issn = {1389-2576, 1573-7632},
  doi = {10.1007/s10710-014-9218-0},
  abstract = {Geometric semantic operators are new and promising genetic operators for genetic programming. They have the property of inducing a unimodal error surface for any supervised learning problem, i.e., any problem consisting in finding the match between a set of input data and known target values (like regression and classification). Thanks to an efficient implementation of these operators, it was possible to apply them to a set of real-life problems, obtaining very encouraging results. We have now made this implementation publicly available as open source software, and here we describe how to use it. We also reveal details of the implementation and perform an investigation of its efficiency in terms of running time and memory occupation, both theoretically and experimentally. The source code and documentation are available for download at http://gsgp.sourceforge.net.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/media/bill/Drive/zotero/data/storage/BGISTBVJ/Castelli et al. - 2015 - A C++ framework for geometric semantic genetic pro.pdf;/media/bill/Drive/zotero/data/storage/DQUP2AZ7/10.html},
  journal = {Genetic Programming and Evolvable Machines},
  language = {en},
  number = {1}
}

@inproceedings{chenXgboostScalableTree2016,
  title = {Xgboost: {{A}} Scalable Tree Boosting System},
  booktitle = {Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  pages = {785--794},
  publisher = {{ACM}}
}

@article{defrancaInteractionTransformationEvolutionaryAlgorithm2020,
  title = {Interaction-{{Transformation Evolutionary Algorithm}} for {{Symbolic Regression}}},
  author = {{de Franca}, F. O. and Aldeia, G. S. I.},
  year = {2020},
  month = dec,
  pages = {1--25},
  issn = {1063-6560},
  doi = {10.1162/evco_a_00285},
  abstract = {Interaction-Transformation (IT) is a new representation for Symbolic Regression that reduces the space of solutions to a set of expressions that follow a specific structure. The potential of this representation was illustrated in prior work with the algorithm called SymTree. This algorithm starts with a simple linear model and incrementally introduces new transformed features until a stop criterion is met. While the results obtained by this algorithm were competitive with the literature, it had the drawback of not scaling well with the problem dimension. This paper introduces a mutation only Evolutionary Algorithm, called ITEA, capable of evolving a population of IT expressions. One advantage of this algorithm is that it enables the user to specify the maximum number of terms in an expression. In order to verify the competitiveness of this approach, ITEA is compared to linear, nonlinear and Symbolic Regression models from the literature. The results indicate that ITEA is capable of finding equal or better approximations than other Symbolic Regression models while being competitive to state-of-the-art non-linear models. Additionally, since this representation follows a specific structure, it is possible to extract the importance of each original feature of a data set as an analytical function, enabling us to automate the explanation of any prediction. In conclusion, ITEA is competitive when comparing to regression models with the additional benefit of automating the extraction of additional information of the generated models.},
  file = {/media/bill/Drive/zotero/data/storage/PVY67AFL/de Franca and Aldeia - 2020 - Interaction-Transformation Evolutionary Algorithm .pdf;/media/bill/Drive/zotero/data/storage/VWS7XT8Y/de Franca and Aldeia - 2020 - Interaction-Transformation Evolutionary Algorithm .pdf;/media/bill/Drive/zotero/data/storage/8IZNBT8W/Interaction-Transformation-Evolutionary-Algorithm.html},
  journal = {Evolutionary Computation}
}

@inproceedings{dickReExaminationUseGenetic2015,
  title = {A {{Re}}-{{Examination}} of the {{Use}} of {{Genetic Programming}} on the {{Oral Bioavailability Problem}}},
  author = {Dick, Grant and Rimoni, Aysha P. and Whigham, Peter A.},
  year = {2015},
  pages = {1015--1022},
  publisher = {{ACM Press}},
  doi = {10.1145/2739480.2754771},
  file = {/media/bill/Drive/zotero/data/storage/MSC98KIQ/p1015.pdf},
  isbn = {978-1-4503-3472-3},
  language = {en}
}

@article{domingosFewUsefulThings2012,
  title = {A Few Useful Things to Know about Machine Learning},
  author = {Domingos, Pedro},
  year = {2012},
  volume = {55},
  pages = {78--87},
  file = {/media/bill/Drive/zotero/data/storage/MAJ4TJXB/ml-survey.pdf},
  journal = {Communications of the ACM},
  number = {10}
}

@inproceedings{druckerImprovingRegressorsUsing1997,
  title = {Improving Regressors Using Boosting Techniques},
  booktitle = {{{ICML}}},
  author = {Drucker, Harris},
  year = {1997},
  volume = {97},
  pages = {107--115}
}

@article{drummondWarningStatisticalBenchmarking2010a,
  title = {Warning: Statistical Benchmarking Is Addictive. {{Kicking}} the Habit in Machine Learning},
  shorttitle = {Warning},
  author = {Drummond, Chris and Japkowicz, Nathalie},
  year = {2010},
  month = mar,
  volume = {22},
  pages = {67--80},
  issn = {0952-813X, 1362-3079},
  doi = {10.1080/09528130903010295},
  journal = {Journal of Experimental \& Theoretical Artificial Intelligence},
  language = {en},
  number = {1}
}

@article{efronLeastAngleRegression2004a,
  title = {Least Angle Regression},
  author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert and others},
  year = {2004},
  volume = {32},
  pages = {407--499},
  publisher = {{Institute of Mathematical Statistics}},
  journal = {The Annals of statistics},
  number = {2}
}

@article{fernandez-delgadoWeNeedHundreds2014a,
  title = {Do We Need Hundreds of Classifiers to Solve Real World Classification Problems},
  author = {{Fern{\'a}ndez-Delgado}, Manuel and Cernadas, Eva and Barro, Sen{\'e}n and Amorim, Dinani},
  year = {2014},
  volume = {15},
  pages = {3133--3181},
  journal = {J. Mach. Learn. Res},
  number = {1}
}

@book{feynmanFeynmanLecturesPhysics2015,
  title = {The {{Feynman Lectures}} on {{Physics}}, {{Vol}}. {{I}}: {{The New Millennium Edition}}: {{Mainly Mechanics}}, {{Radiation}}, and {{Heat}}},
  shorttitle = {The {{Feynman Lectures}} on {{Physics}}, {{Vol}}. {{I}}},
  author = {Feynman, Richard P. and Leighton, Robert B. and Sands, Matthew},
  year = {2015},
  month = sep,
  publisher = {{Basic Books}},
  abstract = {"The whole thing was basically an experiment," Richard Feynman said late in his career, looking back on the origins of his lectures. The experiment turned out to be hugely successful, spawning publications that have remained definitive and introductory to physics for decades. Ranging from the basic principles of Newtonian physics through such formidable theories as general relativity and quantum mechanics, Feynman's lectures stand as a monument of clear exposition and deep insight.Timeless and collectible, the lectures are essential reading, not just for students of physics but for anyone seeking an introduction to the field from the inimitable Feynman.},
  googlebooks = {d76DBQAAQBAJ},
  isbn = {978-0-465-04085-8},
  keywords = {Science / Mechanics / General,Science / Mechanics / Thermodynamics,Science / Physics / Atomic \& Molecular,Science / Physics / General,Science / Radiation},
  language = {en}
}

@article{freundDecisiontheoreticGeneralizationOnline1997,
  title = {A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting},
  author = {Freund, Yoav and Schapire, Robert E},
  year = {1997},
  volume = {55},
  pages = {119--139},
  publisher = {{Elsevier}},
  journal = {Journal of computer and system sciences},
  number = {1}
}

@article{friedmanGreedyFunctionApproximation2001,
  title = {Greedy Function Approximation: A Gradient Boosting Machine},
  author = {Friedman, Jerome H},
  year = {2001},
  pages = {1189--1232},
  publisher = {{JSTOR}},
  journal = {Annals of statistics}
}

@article{hintonConnectionistLearningProcedures1989a,
  title = {Connectionist {{Learning Procedures}}},
  author = {Hinton, Geoffrey E},
  year = {1989},
  volume = {40},
  pages = {185--234},
  journal = {Artificial Intelligence}
}

@inproceedings{hoRandomDecisionForests1995,
  title = {Random Decision Forests},
  booktitle = {Proceedings of 3rd International Conference on Document Analysis and Recognition},
  author = {Ho, Tin Kam},
  year = {1995},
  volume = {1},
  pages = {278--282},
  publisher = {{IEEE}}
}

@inproceedings{hornbyALPSAgelayeredPopulation2006,
  title = {{{ALPS}}: The Age-Layered Population Structure for Reducing the Problem of Premature Convergence},
  shorttitle = {{{ALPS}}},
  booktitle = {Proceedings of the 8th {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Hornby, Gregory S.},
  year = {2006},
  pages = {815--822},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1143997.1144142},
  abstract = {To reduce the problem of premature convergence we define a new method for measuring an individual's age and propose the Age-Layered Population Structure (ALPS). This new measure of age measures how long the genetic material has been evolving in the population: offspring start with an age of 1 plus the age of their oldest parent instead of starting with an age of 0 as with traditional measures of age. ALPS differs from a typical evolutionary algorithm (EA) by segregating individuals into different age-layers by their age and by regularly introducing new, randomly generated individuals in the youngest layer. The introduction of randomly generated individuals at regular intervals results in an EA that is never completely converged and is always exploring new parts of the fitness landscape. By using age to restrict competition and breeding, younger individuals are able to develop without being dominated by older ones. Analysis of the search behavior of ALPS finds that the offspring of individuals that are randomly generated mid-way through a run are able to move the population out of mediocre local-optima to better parts of the fitness landscape. In comparison against a traditional EA, a multi-start EA and two other EAs with diversity maintenance schemes we find that ALPS produces significantly better designs with a higher reliability than the other EAs.},
  file = {/media/bill/Drive/zotero/data/storage/75IDZ7ZH/document.pdf},
  isbn = {1-59593-186-4},
  keywords = {age,computer-automated design,Evolutionary algorithms,open-ended design,Premature convergence},
  series = {{{GECCO}} '06}
}

@article{jinBayesianSymbolicRegression2020,
  title = {Bayesian {{Symbolic Regression}}},
  author = {Jin, Ying and Fu, Weilin and Kang, Jian and Guo, Jiadong and Guo, Jian},
  year = {2020},
  month = jan,
  abstract = {Interpretability is crucial for machine learning in many scenarios such as quantitative finance, banking, healthcare, etc. Symbolic regression (SR) is a classic interpretable machine learning method by bridging X and Y using mathematical expressions composed of some basic functions. However, the search space of all possible expressions grows exponentially with the length of the expression, making it infeasible for enumeration. Genetic programming (GP) has been traditionally and commonly used in SR to search for the optimal solution, but it suffers from several limitations, e.g. the difficulty in incorporating prior knowledge; overly-complicated output expression and reduced interpretability etc. To address these issues, we propose a new method to fit SR under a Bayesian framework. Firstly, Bayesian model can naturally incorporate prior knowledge (e.g., preference of basis functions, operators and raw features) to improve the efficiency of fitting SR. Secondly, to improve interpretability of expressions in SR, we aim to capture concise but informative signals. To this end, we assume the expected signal has an additive structure, i.e., a linear combination of several concise expressions, whose complexity is controlled by a well-designed prior distribution. In our setup, each expression is characterized by a symbolic tree, and the proposed SR model could be solved by sampling symbolic trees from the posterior distribution using an efficient Markov chain Monte Carlo (MCMC) algorithm. Finally, compared with GP, the proposed BSR(Bayesian Symbolic Regression) method saves computer memory with no need to keep an updated 'genome pool'. Numerical experiments show that, compared with GP, the solutions of BSR are closer to the ground truth and the expressions are more concise. Meanwhile we find the solution of BSR is robust to hyper-parameter specifications such as the number of trees.},
  archiveprefix = {arXiv},
  eprint = {1910.08892},
  eprinttype = {arxiv},
  file = {/media/bill/Drive/zotero/data/storage/XTUPVJBS/Jin et al. - 2020 - Bayesian Symbolic Regression.pdf;/media/bill/Drive/zotero/data/storage/QHS3ND2B/1910.html},
  journal = {arXiv:1910.08892 [stat]},
  keywords = {Statistics - Methodology},
  primaryclass = {stat}
}

@article{kaznatcheevComputationalComplexityUltimate2019a,
  title = {Computational {{Complexity}} as an {{Ultimate Constraint}} on {{Evolution}}},
  author = {Kaznatcheev, Artem},
  year = {2019},
  month = may,
  volume = {212},
  pages = {245--265},
  publisher = {{Genetics}},
  issn = {0016-6731, 1943-2631},
  doi = {10.1534/genetics.119.302000},
  abstract = {Experiments show that evolutionary fitness landscapes can have a rich combinatorial structure due to epistasis. For some landscapes, this structure can produce a computational constraint that prevents evolution from finding local fitness optima\textemdash thus overturning the traditional assumption that local fitness peaks can always be reached quickly if no other evolutionary forces challenge natural selection. Here, I introduce a distinction between easy landscapes of traditional theory where local fitness peaks can be found in a moderate number of steps, and hard landscapes where finding local optima requires an infeasible amount of time. Hard examples exist even among landscapes with no reciprocal sign epistasis; on these semismooth fitness landscapes, strong selection weak mutation dynamics cannot find the unique peak in polynomial time. More generally, on hard rugged fitness landscapes that include reciprocal sign epistasis, no evolutionary dynamics\textemdash even ones that do not follow adaptive paths\textemdash can find a local fitness optimum quickly. Moreover, on hard landscapes, the fitness advantage of nearby mutants cannot drop off exponentially fast but has to follow a power-law that long-term evolution experiments have associated with unbounded growth in fitness. Thus, the constraint of computational complexity enables open-ended evolution on finite landscapes. Knowing this constraint allows us to use the tools of theoretical computer science and combinatorial optimization to characterize the fitness landscapes that we expect to see in nature. I present candidates for hard landscapes at scales from single genes, to microbes, to complex organisms with costly learning (Baldwin effect) or maintained cooperation (Hankshaw effect). Just how ubiquitous hard landscapes (and the corresponding ultimate constraint on evolution) are in nature becomes an open empirical question.},
  chapter = {Investigations},
  copyright = {Copyright \textcopyright{} 2019 by the Genetics Society of America},
  file = {/media/bill/Drive/zotero/data/storage/B546KQ3L/Kaznatcheev - 2019 - Computational Complexity as an Ultimate Constraint.pdf;/media/bill/Drive/zotero/data/storage/HLKH4DDF/245.html},
  journal = {Genetics},
  keywords = {computational complexity,evolutionary constraints,fitness landscapes,open-ended evolution,power law},
  language = {en},
  number = {1},
  pmid = {30833289}
}

@article{kearnsPreventingFairnessGerrymandering2018,
  title = {Preventing {{Fairness Gerrymandering}}: {{Auditing}} and {{Learning}} for {{Subgroup Fairness}}},
  shorttitle = {Preventing {{Fairness Gerrymandering}}},
  author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  year = {2018},
  month = dec,
  abstract = {The most prevalent notions of fairness in machine learning are statistical definitions: they fix a small collection of pre-defined groups, and then ask for parity of some statistic of the classifier across these groups. Constraints of this form are susceptible to intentional or inadvertent "fairness gerrymandering", in which a classifier appears to be fair on each individual group, but badly violates the fairness constraint on one or more structured subgroups defined over the protected attributes. We propose instead to demand statistical notions of fairness across exponentially (or infinitely) many subgroups, defined by a structured class of functions over the protected attributes. This interpolates between statistical definitions of fairness and recently proposed individual notions of fairness, but raises several computational challenges. It is no longer clear how to audit a fixed classifier to see if it satisfies such a strong definition of fairness. We prove that the computational problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is equivalent to the problem of weak agnostic learning, which means it is computationally hard in the worst case, even for simple structured subclasses. We then derive two algorithms that provably converge to the best fair classifier, given access to oracles which can solve the agnostic learning problem. The algorithms are based on a formulation of subgroup fairness as a two-player zero-sum game between a Learner and an Auditor. Our first algorithm provably converges in a polynomial number of steps. Our second algorithm enjoys only provably asymptotic convergence, but has the merit of simplicity and faster per-step computation. We implement the simpler algorithm using linear regression as a heuristic oracle, and show that we can effectively both audit and learn fair classifiers on real datasets.},
  archiveprefix = {arXiv},
  eprint = {1711.05144},
  eprinttype = {arxiv},
  file = {/media/bill/Drive/zotero/data/storage/QFZGED9Q/Kearns et al. - 2018 - Preventing Fairness Gerrymandering Auditing and L.pdf;/media/bill/Drive/zotero/data/storage/6CUMTAA4/1711.html},
  journal = {arXiv:1711.05144 [cs]},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{kingmaAdamMethodStochastic2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  month = dec,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  file = {/media/bill/Drive/zotero/data/storage/HBWYR9VF/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf;/media/bill/Drive/zotero/data/storage/N2VC3L3R/1412.html},
  journal = {arXiv:1412.6980 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryclass = {cs}
}

@inproceedings{kommendaEffectsConstantOptimization06,
  title = {Effects of Constant Optimization by Nonlinear Least Squares Minimization in Symbolic Regression},
  booktitle = {{{GECCO}} '13 {{Companion}}: {{Proceeding}} of the Fifteenth Annual Conference Companion on {{Genetic}} and Evolutionary Computation Conference Companion},
  author = {Kommenda, Michael and Kronberger, Gabriel and Winkler, Stephan and Affenzeller, Michael and Wagner, Stefan},
  editor = {Blum, Christian and Alba, Enrique and {Bartz-Beielstein}, Thomas and Loiacono, Daniele and Luna, Francisco and Mehnen, Joern and Ochoa, Gabriela and Preuss, Mike and Tantar, Emilia and Vanneschi, Leonardo},
  year = {6},
  pages = {1121--1128},
  publisher = {{ACM}},
  address = {{Amsterdam, The Netherlands}},
  doi = {doi:10.1145/2464576.2482691},
  abstract = {In this publication a constant optimisation approach for symbolic regression is introduced to separate the task of finding the correct model structure from the necessity to evolve the correct numerical constants. A gradient-based nonlinear least squares optimisation algorithm, the Levenberg-Marquardt (LM) algorithm, is used for adjusting constant values in symbolic expression trees during their evolution. The LM algorithm depends on gradient information consisting of partial derivations of the trees, which are obtained by automatic differentiation. The presented constant optimization approach is tested on several benchmark problems and compared to a standard genetic programming algorithm to show its effectiveness. Although the constant optimization involves an overhead regarding the execution time, the achieved accuracy increases significantly as well as the ability of genetic programming to learn from provided data. As an example, the Pagie-1 problem could be solved in 37 out of 50 test runs, whereas without constant optimisation it was solved in only 10 runs. Furthermore, different configurations of the constant optimisation approach (number of iterations, probability of applying constant optimisation) are evaluated and their impact is detailed in the results section.},
  file = {/media/bill/Drive/zotero/data/storage/GPNVD7TC/Kommenda et al. - 2013 - Effects of constant optimization by nonlinear leas.pdf},
  keywords = {genetic algorithms,genetic programming,gradient methods}
}

@incollection{kommendamichaelEvolvingSimpleSymbolic2015,
  title = {Evolving {{Simple Symbolic Regression Models}} by {{Multi}}-Objective {{Genetic Programming}}},
  booktitle = {Genetic {{Programming Theory}} and {{Practice}}},
  author = {Kommenda, Michael and Kronberger, Gabriel and Affenzeller, Michael and Winkler, Stephan M. and Burlacu, Bogdan},
  year = {2015},
  volume = {XIV},
  publisher = {{Springer}},
  address = {{Ann Arbor, MI}},
  file = {/media/bill/Drive/zotero/data/storage/SPPU5B8X/Kommenda_GPTP.pdf},
  series = {Genetic and {{Evolutionary Computation}}}
}

@article{kommendaParameterIdentificationSymbolic2019,
  ids = {kommendaParameterIdentificationSymbolic},
  title = {Parameter Identification for Symbolic Regression Using Nonlinear Least Squares},
  author = {Kommenda, Michael and Burlacu, Bogdan and Kronberger, Gabriel and Affenzeller, Michael},
  year = {2019},
  month = dec,
  issn = {1573-7632},
  doi = {10.1007/s10710-019-09371-3},
  abstract = {In this paper we analyze the effects of using nonlinear least squares for parameter identification of symbolic regression models and integrate it as local search mechanism in tree-based genetic programming. We employ the Levenberg\textendash Marquardt algorithm for parameter optimization and calculate gradients via automatic differentiation. We provide examples where the parameter identification succeeds and fails and highlight its computational overhead. Using an extensive suite of symbolic regression benchmark problems we demonstrate the increased performance when incorporating nonlinear least squares within genetic programming. Our results are compared with recently published results obtained by several genetic programming variants and state of the art machine learning algorithms. Genetic programming with nonlinear least squares performs among the best on the defined benchmark suite and the local search can be easily integrated in different genetic programming algorithms as long as only differentiable functions are used within the models.},
  file = {/media/bill/Drive/zotero/data/storage/MZ7ICRJR/Kommenda et al. - 2019 - Parameter identification for symbolic regression u.pdf;/media/bill/Drive/zotero/data/storage/JR92A3QH/s10710-019-09371-3.html},
  journal = {Genetic Programming and Evolvable Machines},
  keywords = {Automatic differentiation,Genetic programming,Nonlinear least squares,Parameter identification,Symbolic regression},
  language = {en}
}

@incollection{kornsAccuracySymbolicRegression2011,
  title = {Accuracy in Symbolic Regression},
  booktitle = {Genetic {{Programming Theory}} and {{Practice IX}}},
  author = {Korns, Michael F.},
  year = {2011},
  pages = {129--151},
  publisher = {{Springer}},
  file = {/media/bill/Drive/zotero/data/storage/MBR6TTF6/978-1-4614-1770-5_8.html}
}

@book{kozaGeneticProgrammingProgramming1992a,
  title = {Genetic {{Programming}}: {{On}} the {{Programming}} of {{Computers}} by {{Means}} of {{Natural Selection}}},
  author = {Koza, John R.},
  year = {1992},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  isbn = {0-262-11170-5},
  keywords = {genetic algorithms,text book}
}

@inproceedings{krawiec2013approximating,
  title = {Approximating Geometric Crossover by Semantic Backpropagation},
  booktitle = {Proceedings of the 15th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {Krawiec, Krzysztof and Pawlak, Tomasz},
  year = {2013},
  pages = {941--948}
}

@article{lacavaAutomaticIdentificationWind2016a,
  title = {Automatic Identification of Wind Turbine Models Using Evolutionary Multiobjective Optimization},
  author = {La Cava, William and Danai, Kourosh and Spector, Lee and Fleming, Paul and Wright, Alan and Lackner, Matthew},
  year = {2016},
  month = mar,
  volume = {87},
  pages = {892,902},
  publisher = {{Elsevier}},
  address = {{[New York] :}},
  issn = {0960-1481},
  journal = {Renewable energy},
  lccn = {2004263091}
}

@article{lacavaAutomaticIdentificationWind2016b,
  title = {Automatic Identification of Wind Turbine Models Using Evolutionary Multiobjective Optimization},
  author = {La Cava, William and Danai, Kourosh and Spector, Lee and Fleming, Paul and Wright, Alan and Lackner, Matthew},
  year = {2016},
  month = mar,
  volume = {87, Part 2},
  pages = {892--902},
  issn = {0960-1481},
  doi = {10.1016/j.renene.2015.09.068},
  abstract = {Modern industrial-scale wind turbines are nonlinear systems that operate in turbulent environments. As such, it is difficult to characterize their behavior accurately across a wide range of operating conditions using physically meaningful models. Customarily, the models derived from wind turbine data are in `black box' format, lacking in both conciseness and intelligibility. To address these deficiencies, we use a recently developed symbolic regression method to identify models of a modern horizontal-axis wind turbine in symbolic form. The method uses evolutionary multiobjective optimization to produce succinct dynamic models from operational data while making minimal assumptions about the physical properties of the system. We compare the models produced by this method to models derived by other methods according to their estimation capacity and evaluate the trade-off between model intelligibility and accuracy. Several succinct models are found that predict wind turbine behavior as well as or better than more complex alternatives derived by other methods. We interpret the new models to show that they often contain intelligible estimates of real process physics.},
  journal = {Renewable Energy},
  keywords = {genetic programming,multiobjective optimization,system identification,wind energy},
  series = {Optimization {{Methods}} in {{Renewable Energy Systems Design}}}
}

@inproceedings{lacavaEpsilonLexicaseSelectionRegression2016c,
  title = {Epsilon-{{Lexicase Selection}} for {{Regression}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} 2016},
  author = {La Cava, William and Spector, Lee and Danai, Kourosh},
  year = {2016},
  pages = {741--748},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2908812.2908898},
  abstract = {Lexicase selection is a parent selection method that considers test cases separately, rather than in aggregate, when performing parent selection. It performs well in discrete error spaces but not on the continuous-valued problems that compose most system identification tasks. In this paper, we develop a new form of lexicase selection for symbolic regression, named {$\epsilon$}-lexicase selection, that redefines the pass condition for individuals on each test case in a more effective way. We run a series of experiments on real-world and synthetic problems with several treatments of {$\epsilon$} and quantify how {$\epsilon$} affects parent selection and model performance. {$\epsilon$}-lexicase selection is shown to be effective for regression, producing better fit models compared to other techniques such as tournament selection and age-fitness Pareto optimization. We demonstrate that {$\epsilon$} can be adapted automatically for individual test cases based on the population performance distribution. Our experiments show that {$\epsilon$}-lexicase selection with automatic {$\epsilon$} produces the most accurate models across tested problems with negligible computational overhead. We show that behavioral diversity is exceptionally high in lexicase selection treatments, and that {$\epsilon$}-lexicase selection makes use of more fitness cases when selecting parents than lexicase selection, which helps explain the performance improvement.},
  isbn = {978-1-4503-4206-3},
  keywords = {genetic programming,parent selection,Regression,system identification},
  series = {{{GECCO}} '16}
}

@inproceedings{lacavaEpsilonLexicaseSelectionRegression2016d,
  title = {Epsilon-{{Lexicase Selection}} for {{Regression}}},
  booktitle = {Proceedings of the 2016 on {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {La Cava, William and Spector, Lee and Danai, Kourosh},
  year = {2016},
  pages = {741--748},
  publisher = {{ACM}},
  doi = {10.1145/2908812.2908898},
  archiveprefix = {arXiv},
  eprint = {1905.13266},
  eprinttype = {arxiv},
  keywords = {peerreviewed}
}

@inproceedings{lacavaGeneralFeatureEngineering2017,
  title = {A {{General Feature Engineering Wrapper}} for {{Machine Learning Using}} \textbackslash epsilon -{{Lexicase Survival}}},
  booktitle = {Genetic {{Programming}}},
  author = {La Cava, William and Moore, Jason},
  year = {2017},
  month = apr,
  pages = {80--95},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-55696-3_6},
  abstract = {We propose a general wrapper for feature learning that interfaces with other machine learning methods to compose effective data representations. The proposed feature engineering wrapper (FEW) uses genetic programming to represent and evolve individual features tailored to the machine learning method with which it is paired. In order to maintain feature diversity, {$\epsilon\epsilon\backslash$}epsilon -lexicase survival is introduced, a method based on {$\epsilon\epsilon\backslash$}epsilon -lexicase selection. This survival method preserves semantically unique individuals in the population based on their ability to solve difficult subsets of training cases, thereby yielding a population of uncorrelated features. We demonstrate FEW with five different off-the-shelf machine learning methods and test it on a set of real-world and synthetic regression problems with dimensions varying across three orders of magnitude. The results show that FEW is able to improve model test predictions across problems for several ML methods. We discuss and test the scalability of FEW in comparison to other feature composition strategies, most notably polynomial feature expansion.},
  file = {/media/bill/Drive/zotero/data/storage/KAISKX2V/Cava and Moore - 2017 - A General Feature Engineering Wrapper for Machine .pdf;/media/bill/Drive/zotero/data/storage/6GFEJS8J/978-3-319-55696-3_6.html},
  keywords = {peerreviewed},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{lacavaInferenceCompactNonlinear2016,
  ids = {lacavaInferenceCompactNonlinear2016a,lacavaInferenceCompactNonlinear2016b},
  title = {Inference of Compact Nonlinear Dynamic Models by Epigenetic Local Search},
  author = {La Cava, William and Danai, Kourosh and Spector, Lee},
  year = {2016},
  month = oct,
  volume = {55},
  pages = {292--306},
  publisher = {{Elsevier}},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2016.07.004},
  abstract = {We introduce a method to enhance the inference of meaningful dynamic models from observational data by genetic programming (GP). This method incorporates an inheritable epigenetic layer that specifies active and inactive genes for a more effective local search of the model structure space. We define several GP implementations using different features of epigenetics, such as passive structure, phenotypic plasticity, and inheritable gene regulation. To test these implementations, we use hundreds of data sets generated from nonlinear ordinary differential equations (ODEs) in several fields of engineering and from randomly constructed nonlinear ODE models. The results indicate that epigenetic hill climbing consistently produces more compact dynamic equations with better fitness values, and that it identifies the exact solution of the system more often, validating the categorical improvement of GP by epigenetic local search. The results further indicate that when faced with complex dynamics, epigenetic hill climbing reduces the computational effort required to infer the correct underlying dynamics. We then apply the method to the identification of three real-world systems: a cascaded tanks system, a chemical distillation tower, and an industrial wind turbine. We analyze its solutions in comparison to theoretical and black-box approaches in terms of accuracy and intelligibility. Finally, we analyze population homology to evaluate the efficiency of the method. The results indicate that the epigenetic implementations provide protection from premature convergence by maintaining diversity in silenced portions of programs.},
  file = {/media/bill/Drive/zotero/data/storage/PS3AK47Q/La Cava et al. - 2016 - Inference of compact nonlinear dynamic models by e.pdf;/media/bill/Drive/zotero/data/storage/FJINBRC6/S0952197616301294.html},
  journal = {Engineering Applications of Artificial Intelligence},
  keywords = {Differential equations,dynamical systems,genetic programming,peerreviewed,Symbolic regression,system identification}
}

@inproceedings{lacavaLearningConciseRepresentations2019c,
  title = {Learning Concise Representations for Regression by Evolving Networks of Trees},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {La Cava, William and Moore, Jason H.},
  year = {2019},
  month = may,
  archiveprefix = {arXiv},
  eprint = {1807.00981},
  eprinttype = {arxiv},
  keywords = {highlight,peerreviewed},
  series = {{{ICLR}}}
}

@article{lacavaProbabilisticMultiobjectiveAnalysis2019,
  title = {A Probabilistic and Multi-Objective Analysis of Lexicase Selection and Epsilon-Lexicase Selection},
  author = {La Cava, William and Helmuth, Thomas and Spector, Lee and Moore, Jason H.},
  year = {2019},
  month = sep,
  volume = {27},
  pages = {377--402},
  issn = {1063-6560},
  doi = {10.1162/evco_a_00224},
  file = {/media/bill/Drive/zotero/data/storage/VEUDG34X/La Cava et al. - 2018 - A probabilistic and multi-objective analysis of le.pdf;/media/bill/Drive/zotero/data/storage/XFVVDVP8/evco_a_00224.html},
  journal = {Evolutionary Computation},
  keywords = {peerreviewed},
  number = {3}
}

@book{lichmanUCIMachineLearning2013a,
  title = {{{UCI Machine Learning Repository}}},
  author = {Lichman, M.},
  year = {2013},
  publisher = {{University of California, Irvine, School of Information and Computer Sciences}}
}

@inproceedings{liskowskiComparisonSemanticawareSelection2015,
  title = {Comparison of {{Semantic}}-Aware {{Selection Methods}} in {{Genetic Programming}}},
  booktitle = {Proceedings of the {{Companion Publication}} of the 2015 {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Liskowski, Pawel and Krawiec, Krzysztof and Helmuth, Thomas and Spector, Lee},
  year = {2015},
  pages = {1301--1307},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2739482.2768505},
  abstract = {This study investigates the performance of several semantic- aware selection methods for genetic programming (GP). In particular, we consider methods that do not rely on complete GP semantics (i.e., a tuple of outputs produced by a program for fitness cases (tests)), but on binary outcome vectors that only state whether a given test has been passed by a program or not. This allows us to relate to test-based problems commonly considered in the domain of coevolutionary algorithms and, in prospect, to address a wider range of practical problems, in particular the problems where desired program output is unknown (e.g., evolving GP controllers). The selection methods considered in the paper include implicit fitness sharing (ifs), discovery of derived objectives (doc), lexicase selection (lex), as well as a hybrid of the latter two. These techniques, together with a few variants, are experimentally compared to each other and to conventional GP on a battery of discrete benchmark problems. The outcomes indicate superior performance of lex and ifs, with some variants of doc showing certain potential.},
  file = {/media/bill/Drive/zotero/data/storage/5QSVZSAA/Liskowski et al. - 2015 - Comparison of Semantic-aware Selection Methods in .pdf},
  isbn = {978-1-4503-3488-4},
  keywords = {genetic programming,program semantics,selection operators},
  series = {{{GECCO Companion}} '15}
}

@inproceedings{liskowskiDiscoverySearchObjectives2017,
  title = {Discovery of {{Search Objectives}} in {{Continuous Domains}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Liskowski, Pawe{\textbackslash}l and Krawiec, Krzysztof},
  year = {2017},
  pages = {969--976},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3071178.3071344},
  abstract = {In genetic programming (GP), the outcomes of the evaluation phase can be represented as an interaction matrix, with rows corresponding to programs in a population and columns corresponding to tests that define a program synthesis task. Recent contributions on Discovery of Objectives via Clustering (DOC) and Discovery of Objectives by Factorization of interaction matrix (DOF) show that informative characterizations of programs can be automatically derived from interaction matrices in discrete domains and used as search objectives in multidimensional setting. In this paper, we propose analogous methods for continuous domains and compare them with conventional GP that uses tournament selection, Age-Fitness Pareto Optimization, and GP with epsilon-lexicase selection. Experiments show that the proposed methods are effective for symbolic regression, systematically producing better-fitting models than the two former baselines, and surpassing epsilon-lexicase selection on some problems. We also investigate the hybrids of the proposed approach with the baselines, concluding that hybridization of DOC with epsilon-lexicase leads to the best overall results.},
  file = {/media/bill/Drive/zotero/data/storage/6XSTMUHE/Liskowski and Krawiec - 2017 - Discovery of Search Objectives in Continuous Domai.pdf},
  isbn = {978-1-4503-4920-8},
  keywords = {genetic programming,machine learning,multiobjective optimization,nonnegative matrix factorization},
  series = {{{GECCO}} '17}
}

@inproceedings{lukeECJThenNow2017,
  title = {{{ECJ}} Then and Now},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Luke, Sean},
  year = {2017},
  month = jul,
  pages = {1223--1230},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3067695.3082467},
  abstract = {ECJ is a mature and widely used evolutionary computation library with particular strengths in genetic programming, massive distributed computation, and coevolution. In Fall of 2016 we received a three-year NSF grant to expand ECJ into a toolkit with wide-ranging facilities designed to serve the broader metaheuristics community. This report discusses ECJ's history, capabilities, and architecture, then details our planned extensions and expansions.},
  file = {/media/bill/Drive/zotero/data/storage/KU3G4YXD/Luke - 2017 - ECJ then and now.pdf},
  isbn = {978-1-4503-4939-0},
  keywords = {evolutionary computation,implementation,libraries,metaheuristics},
  series = {{{GECCO}} '17}
}

@article{lyLearningSymbolicRepresentations2012a,
  title = {Learning Symbolic Representations of Hybrid Dynamical Systems},
  author = {Ly, Daniel L. and Lipson, Hod},
  year = {2012},
  volume = {13},
  pages = {3585--3618},
  journal = {Journal of Machine Learning Research},
  number = {Dec}
}

@incollection{mcconaghyFFXFastScalable2011,
  ids = {mcconaghyFfxFastScalable2011},
  title = {{{FFX}}: {{Fast}}, Scalable, Deterministic Symbolic Regression Technology},
  shorttitle = {{{FFX}}},
  booktitle = {Genetic {{Programming Theory}} and {{Practice IX}}},
  author = {McConaghy, Trent},
  year = {2011},
  pages = {235--260},
  publisher = {{Springer}},
  file = {/media/bill/Drive/zotero/data/storage/D47GCH6F/2011-GPTP-FFX-paper.pdf;/media/bill/Drive/zotero/data/storage/X9BP6P83/McConaghy - 2011 - Ffx Fast, scalable, deterministic symbolic regres.pdf;/media/bill/Drive/zotero/data/storage/Z95MCT28/978-1-4614-1770-5_13.html},
  keywords = {genetic algorithms}
}

@inproceedings{mcdermottGeneticProgrammingNeeds2012b,
  title = {Genetic Programming Needs Better Benchmarks},
  booktitle = {Proceedings of the Fourteenth International Conference on {{Genetic}} and Evolutionary Computation Conference},
  author = {McDermott, James and White, David R. and Luke, Sean and Manzoni, Luca and Castelli, Mauro and Vanneschi, Leonardo and Jaskowski, Wojciech and Krawiec, Krzysztof and Harper, Robin and De Jong, Kenneth},
  year = {2012},
  pages = {791--798},
  publisher = {{ACM}}
}

@incollection{moraglioGeometricSemanticGenetic2012a,
  title = {Geometric Semantic Genetic Programming},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}}-{{PPSN XII}}},
  author = {Moraglio, Alberto and Krawiec, Krzysztof and Johnson, Colin G.},
  year = {2012},
  pages = {21--31},
  publisher = {{Springer}}
}

@article{murdochDefinitionsMethodsApplications2019,
  title = {Definitions, Methods, and Applications in Interpretable Machine Learning},
  author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and {Abbasi-Asl}, Reza and Yu, Bin},
  year = {10 2019-10-29},
  volume = {116},
  pages = {22071--22080},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1900654116},
  file = {/media/bill/Drive/zotero/data/storage/ERC9EP99/Murdoch et al. - 2019 - Definitions, methods, and applications in interpre.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {44},
  pmid = {31619572}
}

@article{nguyenSubtreeSemanticGeometric2015a,
  title = {Subtree Semantic Geometric Crossover for Genetic Programming},
  author = {Nguyen, Quang Uy and Pham, Tuan Anh and Nguyen, Xuan Hoai and McDermott, James},
  year = {2015},
  month = oct,
  pages = {1--29},
  issn = {1389-2576, 1573-7632},
  doi = {10.1007/s10710-015-9253-5},
  abstract = {The semantic geometric crossover (SGX) proposed by Moraglio et al. has achieved very promising results and received great attention from researchers, but has a significant disadvantage in the exponential growth in size of the solutions. We propose a crossover operator named subtree semantic geometric crossover (SSGX), with the aim of addressing this issue. It is similar to SGX but uses subtree semantic similarity to approximate the geometric property. We compare SSGX to standard crossover (SC), to SGX, and to other recent semantic-based crossover operators, testing on several symbolic regression problems. Overall our new operator out-performs the other operators on test data performance, and reduces computational time relative to most of them. Further analysis shows that while SGX is rather exploitative, and SC rather explorative, SSGX achieves a balance between the two. A simple method of further enhancing SSGX performance is also demonstrated.},
  journal = {Genetic Programming and Evolvable Machines},
  keywords = {Artificial Intelligence (incl. Robotics),Biomedical Engineering,Compilers,Electrical Engineering,genetic programming,Geometric crossover,Interpreters,Programming Languages,Programming Techniques,Semantics,Software Engineering/Programming and Operating Systems,Symbolic regression},
  language = {en}
}

@inproceedings{olsonDatadrivenAdviceApplying2017,
  title = {Data-Driven {{Advice}} for {{Applying Machine Learning}} to {{Bioinformatics Problems}}},
  booktitle = {Pacific {{Symposium}} on {{Biocomputing}} ({{PSB}})},
  author = {Olson, Randal S. and La Cava, William and Mustahsan, Zairah and Varik, Akshay and Moore, Jason H.},
  year = {2017},
  month = aug,
  abstract = {As the bioinformatics field grows, it must keep pace not only with new data but with new algorithms. Here we contribute a thorough analysis of 13 state-of-the-art, commonly used machine learning algorithms on a set of 165 publicly available classification problems in order to provide data-driven algorithm recommendations to current researchers. We present a number of statistical and visual comparisons of algorithm performance and quantify the effect of model selection and algorithm tuning for each algorithm and dataset. The analysis culminates in the recommendation of five algorithms with hyperparameters that maximize classifier performance across the tested problems, as well as general guidelines for applying machine learning to supervised classification problems.},
  annotation = {*: contributed equally},
  file = {/media/bill/Drive/zotero/data/storage/H2UI43GG/Olson et al. - 2017 - Data-driven Advice for Applying Machine Learning t.pdf;/media/bill/Drive/zotero/data/storage/H74MT6TH/1708.html},
  keywords = {Computer Science - Learning,peerreviewed,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning}
}

@article{olsonPMLBLargeBenchmark2017d,
  title = {{{PMLB}}: {{A Large Benchmark Suite}} for {{Machine Learning Evaluation}} and {{Comparison}}},
  shorttitle = {{{PMLB}}},
  author = {Olson, Randal S. and La Cava, William and Orzechowski, Patryk and Urbanowicz, Ryan J. and Moore, Jason H.},
  year = {2017},
  journal = {BioData Mining}
}

@inproceedings{orzechowskiWhereAreWe2018,
  ids = {orzechowskiWhereAreWe2018a},
  title = {Where Are We Now? {{A}} Large Benchmark Study of Recent Symbolic Regression Methods},
  shorttitle = {Where Are We Now?},
  booktitle = {Proceedings of the 2018 {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Orzechowski, Patryk and La Cava, William and Moore, Jason H.},
  year = {2018},
  month = apr,
  doi = {10.1145/3205455.3205539},
  archiveprefix = {arXiv},
  eprint = {1804.09331},
  eprinttype = {arxiv},
  file = {/media/bill/Drive/zotero/data/storage/SD8AIWW8/Orzechowski et al. - 2018 - Where are we now A large benchmark study of recen.pdf;/media/bill/Drive/zotero/data/storage/D9BP5F4I/1804.html},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,peerreviewed},
  series = {{{GECCO}} '18}
}

@article{pawlak2014semantic,
  title = {Semantic Backpropagation for Designing Search Operators in Genetic Programming},
  author = {Pawlak, Tomasz P and Wieloch, Bartosz and Krawiec, Krzysztof},
  year = {2014},
  volume = {19},
  pages = {326--340},
  publisher = {{IEEE}},
  journal = {IEEE Transactions on Evolutionary Computation},
  number = {3}
}

@article{pedregosaScikitlearnMachineLearning2011a,
  title = {Scikit-Learn: {{Machine}} Learning in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  year = {2011},
  volume = {12},
  pages = {2825--2830},
  journal = {Journal of Machine Learning Research},
  number = {Oct}
}

@inproceedings{petersenDeepSymbolicRegression2020,
  title = {Deep Symbolic Regression: {{Recovering}} Mathematical Expressions from Data via Risk-Seeking Policy Gradients},
  shorttitle = {Deep Symbolic Regression},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Petersen, Brenden K. and Larma, Mikel Landajuela and Mundhenk, Terrell N. and Santiago, Claudio Prata and Kim, Soo Kyung and Kim, Joanne Taery},
  year = {2020},
  month = sep,
  abstract = {Discovering the underlying mathematical expressions describing a dataset is a core challenge for artificial intelligence. This is the problem of symbolic regression. Despite recent advances in...},
  file = {/media/bill/Drive/zotero/data/storage/BDINSBYK/Petersen et al. - 2020 - Deep symbolic regression Recovering mathematical .pdf;/media/bill/Drive/zotero/data/storage/UASTXT4J/forum.html},
  language = {en}
}

@article{poliBuildingBlockBasis2005,
  title = {The Building Block Basis for Genetic Programming and Variable-Length Genetic Algorithms},
  author = {Poli, Riccardo and Stephens, Christopher R.},
  year = {2005},
  volume = {1},
  pages = {183--197},
  file = {/media/bill/Drive/zotero/data/storage/AZZE32M7/Poli and Stephens - 2005 - The building block basis for genetic programming a.pdf},
  journal = {International Journal of Computational Intelligence Research},
  number = {1}
}

@article{poliExactSchemaTheory2004,
  title = {Exact Schema Theory and Markov Chain Models for Genetic Programming and Variable-Length Genetic Algorithms with Homologous Crossover},
  author = {Poli, Riccardo and McPhee, Nicholas Freitag and Rowe, Jonathan E.},
  year = {2004},
  volume = {5},
  pages = {31--70},
  file = {/media/bill/Drive/zotero/data/storage/XJWJABJK/Poli et al. - 2004 - Exact schema theory and markov chain models for ge.pdf;/media/bill/Drive/zotero/data/storage/XWVWXKL3/citation.html},
  journal = {Genetic Programming and Evolvable Machines},
  number = {1}
}

@book{poliFieldGuideGenetic2008,
  title = {A Field Guide to Genetic Programming},
  author = {Poli, Riccardo and McPhee, Nicholas F and Koza, John R},
  year = {2008},
  publisher = {{[Lulu Press], lulu.com}},
  address = {{[S.I.]}},
  annotation = {ZSCC: 0001828},
  file = {/media/bill/Drive/zotero/data/storage/4SDTTZAN/Field guide to genetic programming.pdf;/media/bill/Drive/zotero/data/storage/7RZHAJ7Q/Field guide to genetic programming.pdf},
  isbn = {978-1-4092-0073-4 1-4092-0073-6},
  keywords = {genetic algorithms,genetic programming},
  language = {English}
}

@article{poliGeneralSchemaTheory2003,
  title = {General Schema Theory for Genetic Programming with Subtree-Swapping Crossover: {{Part I}}},
  shorttitle = {General Schema Theory for Genetic Programming with Subtree-Swapping Crossover},
  author = {Poli, Riccardo and McPhee, Nicholas Freitag},
  year = {2003},
  volume = {11},
  pages = {53--66},
  file = {/media/bill/Drive/zotero/data/storage/UK4AQ627/Poli and McPhee - 2003 - General schema theory for genetic programming with.pdf;/media/bill/Drive/zotero/data/storage/ZQ53FMG6/106365603321829005.html},
  journal = {Evolutionary Computation},
  number = {1}
}

@article{poliGeneralSchemaTheory2003a,
  title = {General Schema Theory for Genetic Programming with Subtree-Swapping Crossover: {{Part II}}},
  shorttitle = {General Schema Theory for Genetic Programming with Subtree-Swapping Crossover},
  author = {Poli, Riccardo and McPhee, Nicholas Freitag},
  year = {2003},
  volume = {11},
  pages = {169--206},
  file = {/media/bill/Drive/zotero/data/storage/87AHR8AB/Poli and McPhee - 2003 - General schema theory for genetic programming with.pdf;/media/bill/Drive/zotero/data/storage/DW3TGDSG/106365603766646825.html},
  journal = {Evolutionary Computation},
  number = {2}
}

@article{poliTheoreticalResultsGenetic2010,
  title = {Theoretical Results in Genetic Programming: The next Ten Years?},
  shorttitle = {Theoretical Results in Genetic Programming},
  author = {Poli, Riccardo and Vanneschi, Leonardo and Langdon, William B. and McPhee, Nicholas Freitag},
  year = {2010},
  volume = {11},
  pages = {285--320},
  file = {/media/bill/Drive/zotero/data/storage/25N6FYQP/Poli et al. - 2010 - Theoretical results in genetic programming the ne.pdf;/media/bill/Drive/zotero/data/storage/9C2JJ5QG/s10710-010-9110-5.html;/media/bill/Drive/zotero/data/storage/MGQXBN65/s10710-010-9110-5.html},
  journal = {Genetic Programming and Evolvable Machines},
  number = {3-4}
}

@book{robertMachineLearningProbabilistic2014,
  title = {Machine Learning, a Probabilistic Perspective},
  author = {Robert, Christian},
  year = {2014},
  publisher = {{Taylor \& Francis}}
}

@article{romanoPMLBV1Open2021,
  title = {{{PMLB}} v1.0: {{An}} Open Source Dataset Collection for Benchmarking Machine Learning Methods},
  shorttitle = {{{PMLB}} v1.0},
  author = {Romano, Joseph D. and Le, Trang T. and La Cava, William and Gregg, John T. and Goldberg, Daniel J. and Ray, Natasha L. and Chakraborty, Praneel and Himmelstein, Daniel and Fu, Weixuan and Moore, Jason H.},
  year = {2021},
  month = apr,
  abstract = {Motivation: Novel machine learning and statistical modeling studies rely on standardized comparisons to existing methods using well-studied benchmark datasets. Few tools exist that provide rapid access to many of these datasets through a standardized, user-friendly interface that integrates well with popular data science workflows. Results: This release of PMLB provides the largest collection of diverse, public benchmark datasets for evaluating new machine learning and data science methods aggregated in one location. v1.0 introduces a number of critical improvements developed following discussions with the open-source community. Availability: PMLB is available at https://github.com/EpistasisLab/pmlb. Python and R interfaces for PMLB can be installed through the Python Package Index and Comprehensive R Archive Network, respectively.},
  archiveprefix = {arXiv},
  eprint = {2012.00058},
  eprinttype = {arxiv},
  file = {/media/bill/Drive/zotero/data/storage/CXULSX5A/Romano et al. - 2021 - PMLB v1.0 An open source dataset collection for b.pdf;/media/bill/Drive/zotero/data/storage/RHVECCF4/2012.html},
  journal = {arXiv:2012.00058 [cs]},
  keywords = {Computer Science - Databases,Computer Science - Machine Learning,H.2.8},
  primaryclass = {cs}
}

@incollection{schmidtAgefitnessParetoOptimization2011,
  title = {Age-Fitness Pareto Optimization},
  booktitle = {Genetic {{Programming Theory}} and {{Practice VIII}}},
  author = {Schmidt, Michael and Lipson, Hod},
  year = {2011},
  pages = {129--146},
  publisher = {{Springer}},
  file = {/media/bill/Drive/zotero/data/storage/38MNN8KW/Schmidt-20100712.pdf}
}

@inproceedings{schmidtAutomatedModelingStochastic2011,
  title = {Automated Modeling of Stochastic Reactions with Large Measurement Time-Gaps},
  booktitle = {Proceedings of the 13th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {Schmidt, Michael Douglas and Lipson, Hod},
  year = {2011},
  pages = {307--314},
  publisher = {{ACM}},
  file = {/media/bill/Drive/zotero/data/storage/ND8MW95N/Schmidt_Lipson_2011_Automated modeling of stochastic reactions with large measurement time-gaps.pdf;/media/bill/Drive/zotero/data/storage/H5ZNJUWT/citation.html}
}

@article{schmidtAutomatedRefinementInference2011a,
  ids = {schmidtAutomatedRefinementInference2011,schmidtAutomatedRefinementInference2011b},
  title = {Automated Refinement and Inference of Analytical Models for Metabolic Networks},
  author = {Schmidt, Michael D and Vallabhajosyula, Ravishankar R and Jenkins, Jerry W and Hood, Jonathan E and Soni, Abhishek S and Wikswo, John P and Lipson, Hod},
  year = {2011},
  month = oct,
  volume = {8},
  pages = {055011},
  issn = {1478-3975},
  doi = {10.1088/1478-3975/8/5/055011},
  file = {/media/bill/Drive/zotero/data/storage/65856I58/Schmidt et al_2011_Automated refinement and inference of analytical models for metabolic networks.pdf;/media/bill/Drive/zotero/data/storage/9UTUPEH9/Schmidt et al. - 2011 - Automated refinement and inference of analytical m.pdf;/media/bill/Drive/zotero/data/storage/2GTTMZXW/461.html},
  journal = {Physical Biology},
  number = {5}
}

@article{schmidtCoevolutionFitnessPredictors2008,
  title = {Coevolution of {{Fitness Predictors}}},
  author = {Schmidt, M.D. and Lipson, H.},
  year = {2008},
  month = dec,
  volume = {12},
  pages = {736--749},
  issn = {1941-0026, 1089-778X},
  doi = {10.1109/TEVC.2008.919006},
  file = {/media/bill/Drive/zotero/data/storage/B6MUFKJ8/Schmidt - Coevolution of Fitness Predictors.pdf},
  journal = {IEEE Transactions on Evolutionary Computation},
  number = {6}
}

@incollection{schmidtCoevolvingFitnessModels2007,
  title = {Coevolving Fitness Models for Accelerating Evolution and Reducing Evaluations},
  booktitle = {Genetic {{Programming Theory}} and {{Practice IV}}},
  author = {Schmidt, Michael D. and Lipson, Hod},
  year = {2007},
  pages = {113--130},
  publisher = {{Springer}},
  file = {/media/bill/Drive/zotero/data/storage/5VCSDQ26/978-0-387-49650-4_8.html}
}

@inproceedings{schmidtComparisonTreeGraph2007,
  title = {Comparison of {{Tree}} and {{Graph Encodings As Function}} of {{Problem Complexity}}},
  booktitle = {Proceedings of the 9th {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Schmidt, Michael and Lipson, Hod},
  year = {2007},
  pages = {1674--1679},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1276958.1277288},
  abstract = {In this paper, we analyze two general-purpose encoding types, trees and graphs systematically, focusing on trends over increasingly complex problems. Tree and graph encodings are similar in application but offer distinct advantages and disadvantages in genetic programming. We describe two implementations and discuss their evolvability. We then compare performance using symbolic regression on hundreds of random nonlinear target functions of both 1-dimensional and 8-dimensional cases. Results show the graph encoding has less bias for bloating solutions but is slower to converge and deleterious crossovers are more frequent. The graph encoding however is found to have computational benefits, suggesting it to be an advantageous trade-off between regression performance and computational effort.},
  file = {/media/bill/Drive/zotero/data/storage/SZZ5DSS7/Schmidt_Lipson_2007_Comparison of Tree and Graph Encodings As Function of Problem Complexity.pdf},
  isbn = {978-1-59593-697-4},
  keywords = {expression graphs,expression trees,Symbolic regression},
  series = {{{GECCO}} '07}
}

@article{schmidtDistillingFreeformNatural2009,
  title = {Distilling Free-Form Natural Laws from Experimental Data},
  author = {Schmidt, Michael and Lipson, Hod},
  year = {2009},
  volume = {324},
  pages = {81--85},
  journal = {Science},
  number = {5923}
}

@article{schmidtDistillingFreeformNatural2009a,
  title = {Distilling Free-Form Natural Laws from Experimental Data},
  author = {Schmidt, Michael and Lipson, Hod},
  year = {2009},
  volume = {324},
  pages = {81--85},
  file = {/media/bill/Drive/zotero/data/storage/4R4F5MBZ/Schmidt - Distilling Free Form Natural Laws from Experimental Data.pdf;/media/bill/Drive/zotero/data/storage/8C2VR7GF/Schmidt - Supporting Online Material for distilling free-form natural laws.pdf},
  journal = {Science},
  keywords = {genetic algorithms},
  number = {5923}
}

@article{schmidtDistillingFreeformNatural2009b,
  title = {Distilling Free-Form Natural Laws from Experimental Data},
  author = {Schmidt, Michael and Lipson, Hod},
  year = {2009},
  volume = {324},
  pages = {81--85},
  journal = {Science},
  keywords = {genetic algorithms},
  number = {5923}
}

@inproceedings{schmidtIncorporatingExpertKnowledge2009,
  title = {Incorporating Expert Knowledge in Evolutionary Search: A Study of Seeding Methods},
  shorttitle = {Incorporating Expert Knowledge in Evolutionary Search},
  booktitle = {Proceedings of the 11th {{Annual}} Conference on {{Genetic}} and Evolutionary Computation},
  author = {Schmidt, Michael D. and Lipson, Hod},
  year = {2009},
  pages = {1091--1098},
  publisher = {{ACM}},
  file = {/media/bill/Drive/zotero/data/storage/ZFEVT8CG/Schmidt and Lipson - 2009 - Incorporating expert knowledge in evolutionary sea.pdf;/media/bill/Drive/zotero/data/storage/KEGK37EK/citation.html}
}

@inproceedings{schmidtLearningNoise2007,
  title = {Learning {{Noise}}},
  booktitle = {Proceedings of the 9th {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Schmidt, Michael D. and Lipson, Hod},
  year = {2007},
  pages = {1680--1685},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1276958.1277289},
  abstract = {In this paper we propose a genetic programming approach to learning stochastic models with unsymmetrical noise distributions. Most learning algorithms try to learn from noisy data by modeling the maximum likelihood output or least squared error, assuming that noise effects average out. While this process works well for data with symmetrical noise distributions (such as Gaussian observation noise), many real-life sources of noise are not symmetrically distributed, thus this approach does not hold. We suggest improved learning can be obtained by including noise sources explicitly in the model as a stochastic element. A stochastic element is a random sub-process or latent variable of a hidden system that can propagate nonlinear noise to the observable outputs. Stochastic elements can skew and distort output features making regression of analytical models particularly difficult and error minimizing approaches inhibiting. We introduce a new method to infer the analytical model of a system by decomposing non-uniform noise observed at the outputs into uniform stochastic elements appearing symbolically inside the system. Results demonstrate the ability to regress exact analytical models where stochastic elements are embedded inside nonlinear and polynomial hidden systems.},
  file = {/media/bill/Drive/zotero/data/storage/26A7W8B6/Schmidt and Lipson - 2007 - Learning Noise.pdf},
  isbn = {978-1-59593-697-4},
  keywords = {dynamical systems,stochastic elements,Symbolic regression},
  series = {{{GECCO}} '07}
}

@phdthesis{schmidtMachineScienceAutomated2011,
  title = {Machine {{Science}}: {{Automated Modeling}} of {{Deterministic}} and {{Stochastic Dynamical Systems}}},
  shorttitle = {Machine {{Science}}},
  author = {Schmidt, Michael Douglas},
  year = {2011},
  address = {{Ithaca, NY, USA}},
  abstract = {The work presented here advances the technology to analyze experimental data and automatically hypothesize about explanatory models and physical laws that help explain observations. Automated Modeling, sometimes referred to as Symbolic Regression or System Identification, is the process of searching a possibly infinite space of mathematical expressions in order to optimize various objectives\textemdash for example, identifying the simplest possible nonlinear equation that captures the observed dynamics of a system. Traditionally, the task of formulating analytical models and theory has remained entirely within the purview of human expertise, and also human limitation. However, the development of Evolutionary Algorithms, and more recently Genetic Programming, has made searching for analytical models automatically a possibility. The work presented here focuses on advancing the algorithms and techniques for Automated Modeling to shrink this ``reality gap,'' and applies these advances to various real and experimental systems for the first time. The specific contributions of this work fall into four categories: search methods and algorithms, model representations and the types of systems that can be analyzed, techniques for interpreting solutions and results, and applications in science and engineering fields. The most important contribution in the search methods is the Fitness and Rank Prediction algorithm, which enables utilizing exceedingly large data sets with low computational effort. This algorithm is based on the idea that, at any given time, only a small number of carefully selected data points are necessary to discriminate among candidate models, allowing large reductions in computational effort. In model representations, the most important contribution is the principle for identifying meaningful invariant quantities amongst the infinite number of trivial invariant expressions. This principle enables searching for physical laws and conservations directly from experimental measurements. In the interpretation of results, the most important contribution is Parameter Mapping technique, which relates an automatically inferred model to a previous model through repeated regressions. Finally, the most important contribution in applications is the analysis of yeast Glycolytic oscillations, which demonstrates and compares several techniques in order to identify a complete nonlinear ordinary differential equation model directly from data.},
  annotation = {AAI3484909},
  school = {Cornell University}
}

@inproceedings{schmidtPredictingSolutionRank2010,
  title = {Predicting Solution Rank to Improve Performance},
  booktitle = {Proceedings of the 12th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {Schmidt, Michael D. and Lipson, Hod},
  year = {2010},
  pages = {949--956},
  file = {/media/bill/Drive/zotero/data/storage/5Z2XDBGJ/Schmidt - Predicting solution rank to improve performance.pdf}
}

@inproceedings{schmidtPredictingSolutionRank2010a,
  title = {Predicting Solution Rank to Improve Performance},
  booktitle = {Proceedings of the 12th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {Schmidt, Michael D. and Lipson, Hod},
  year = {2010},
  pages = {949--956},
  publisher = {{ACM}},
  file = {/media/bill/Drive/zotero/data/storage/4QWJCZHI/Schmidt and Lipson - 2010 - Predicting solution rank to improve performance.pdf;/media/bill/Drive/zotero/data/storage/GJ8RQ2R5/citation.html}
}

@incollection{schmidtSymbolicRegressionImplicit2010,
  title = {Symbolic Regression of Implicit Equations},
  booktitle = {Genetic {{Programming Theory}} and {{Practice VII}}},
  author = {Schmidt, Michael and Lipson, Hod},
  year = {2010},
  pages = {73--85},
  publisher = {{Springer}},
  file = {/media/bill/Drive/zotero/data/storage/A2QS47IM/Schmidt_Lipson_2010_Symbolic regression of implicit equations.pdf;/media/bill/Drive/zotero/data/storage/NSBHMZ4I/978-1-4419-1626-6_5.html}
}

@incollection{schmidtSymbolicRegressionImplicit2010a,
  title = {Symbolic Regression of Implicit Equations},
  booktitle = {Genetic {{Programming Theory}} and {{Practice VII}}},
  author = {Schmidt, Michael and Lipson, Hod},
  year = {2010},
  pages = {73--85},
  publisher = {{Springer}},
  file = {/media/bill/Drive/zotero/data/storage/3X3Z4MPF/Schmidt_Lipson_2010_Symbolic regression of implicit equations.pdf;/media/bill/Drive/zotero/data/storage/3MQ6X9RQ/978-1-4419-1626-6_5.html}
}

@article{smolaTutorialSupportVector2004,
  title = {A Tutorial on Support Vector Regression},
  author = {Smola, Alex J and Sch{\"o}lkopf, Bernhard},
  year = {2004},
  volume = {14},
  pages = {199--222},
  publisher = {{Springer}},
  journal = {Statistics and computing},
  number = {3}
}

@inproceedings{spectorAssessmentProblemModality2012,
  title = {Assessment of Problem Modality by Differential Performance of Lexicase Selection in Genetic Programming: A Preliminary Report},
  shorttitle = {Assessment of Problem Modality by Differential Performance of Lexicase Selection in Genetic Programming},
  booktitle = {Proceedings of the Fourteenth International Conference on {{Genetic}} and Evolutionary Computation Conference Companion},
  author = {Spector, Lee},
  year = {2012},
  pages = {401--408},
  file = {/media/bill/Drive/zotero/data/storage/AFHJU5T3/Spector - 2012 - Assessment of problem modality by differential per.pdf}
}

@article{stanislawskaModelingGlobalTemperature2012a,
  title = {Modeling Global Temperature Changes with Genetic Programming},
  author = {Stanislawska, Karolina and Krawiec, Krzysztof and Kundzewicz, Zbigniew W.},
  year = {2012},
  month = dec,
  volume = {64},
  pages = {3717--3728},
  issn = {0898-1221},
  doi = {10.1016/j.camwa.2012.02.049},
  abstract = {We use genetic programming (GP), a variant of evolutionary computation, to build interpretable models of global mean temperature as a function of natural and anthropogenic forcings. In contrast to the conventional approach, which engages models that are physically-based but very data-demanding and computation-intense, the proposed method is a data-driven randomized search algorithm capable of inducing a model from moderate amount of training data at reasonable computational cost. GP maintains a population of models and recombines them iteratively to improve their performance meant as an ability to explain the training data. Each model is a multiple input\textendash single output arithmetic expression built of a predefined set of elementary components. Inputs include external climate forcings, such as solar activity, volcanic eruptions, composition of the atmosphere (greenhouse gas concentration and aerosols), and indices of internal variability (oscillations in the Ocean-Atmosphere system), while the output is the large-scale temperature. We used the data from the period 1900\textendash 1999 for training and the period 2000\textendash 2009 for testing, and employed two quality measures: mean absolute error and correlation coefficient. The experiment showed that the models evolved by GP are capable to predict, based exclusively on non-temperature data, the global temperature more accurately than a reference approach known in the literature.},
  journal = {Computers \& Mathematics with Applications},
  keywords = {Data-driven modeling,Evolutionary computation,genetic programming,Global temperature modeling,Unconstrained optimization},
  number = {12},
  series = {Theory and {{Practice}} of {{Stochastic Modeling}}}
}

@book{strogatzNonlinearDynamicsChaos2014,
  title = {Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry, and Engineering},
  author = {Strogatz, Steven H},
  year = {2014},
  publisher = {{Westview press}}
}

@inproceedings{thierens2011optimal,
  title = {Optimal Mixing Evolutionary Algorithms},
  booktitle = {Proceedings of the 13th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {Thierens, Dirk and Bosman, Peter AN},
  year = {2011},
  pages = {617--624}
}

@article{tibshiraniRegressionShrinkageSelection1996a,
  title = {Regression Shrinkage and Selection via the Lasso},
  author = {Tibshirani, Robert},
  year = {1996},
  pages = {267--288},
  publisher = {{JSTOR}},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)}
}

@inproceedings{topchyFasterGeneticProgramming2001,
  title = {Faster Genetic Programming Based on Local Gradient Search of Numeric Leaf Values},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} ({{GECCO}}-2001)},
  author = {Topchy, Alexander and Punch, William F.},
  year = {2001},
  pages = {155--162},
  file = {/media/bill/Drive/zotero/data/storage/2IH9SBNS/Topchy - GP with local gradient search.pdf},
  keywords = {genetic algorithms,gradient methods,hill climbing}
}

@article{udrescuAIFeynmanParetooptimal2020,
  title = {{{AI Feynman}} 2.0: {{Pareto}}-Optimal Symbolic Regression Exploiting Graph Modularity},
  shorttitle = {{{AI Feynman}} 2.0},
  author = {Udrescu, Silviu-Marian and Tan, Andrew and Feng, Jiahai and Neto, Orisvaldo and Wu, Tailin and Tegmark, Max},
  year = {2020},
  month = dec,
  abstract = {We present an improved method for symbolic regression that seeks to fit data to formulas that are Pareto-optimal, in the sense of having the best accuracy for a given complexity. It improves on the previous state-of-the-art by typically being orders of magnitude more robust toward noise and bad data, and also by discovering many formulas that stumped previous methods. We develop a method for discovering generalized symmetries (arbitrary modularity in the computational graph of a formula) from gradient properties of a neural network fit. We use normalizing flows to generalize our symbolic regression method to probability distributions from which we only have samples, and employ statistical hypothesis testing to accelerate robust brute-force search.},
  archiveprefix = {arXiv},
  eprint = {2006.10782},
  eprinttype = {arxiv},
  file = {/media/bill/Drive/zotero/data/storage/9EX5DR2U/Udrescu et al. - 2020 - AI Feynman 2.0 Pareto-optimal symbolic regression.pdf;/media/bill/Drive/zotero/data/storage/NDR2BPRS/2006.html},
  journal = {arXiv:2006.10782 [physics, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Machine Learning,Physics - Computational Physics,Statistics - Machine Learning},
  primaryclass = {physics, stat}
}

@article{udrescuAIFeynmanPhysicsInspired2020,
  title = {{{AI Feynman}}: A {{Physics}}-{{Inspired Method}} for {{Symbolic Regression}}},
  shorttitle = {{{AI Feynman}}},
  author = {Udrescu, Silviu-Marian and Tegmark, Max},
  year = {2020},
  month = apr,
  abstract = {A core challenge for both physics and artificial intellicence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult test set, we improve the state of the art success rate from 15\% to 90\%.},
  archiveprefix = {arXiv},
  eprint = {1905.11481},
  eprinttype = {arxiv},
  file = {/media/bill/Drive/zotero/data/storage/UM239YGL/Udrescu and Tegmark - 2020 - AI Feynman a Physics-Inspired Method for Symbolic.pdf;/media/bill/Drive/zotero/data/storage/CVKC2JX9/1905.html},
  journal = {arXiv:1905.11481 [hep-th, physics:physics]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,High Energy Physics - Theory,Physics - Computational Physics},
  primaryclass = {hep-th, physics:physics}
}

@article{vanneschiSurveySemanticMethods2014,
  title = {A Survey of Semantic Methods in Genetic Programming},
  author = {Vanneschi, Leonardo and Castelli, Mauro and Silva, Sara},
  year = {2014},
  volume = {15},
  pages = {195--214},
  file = {/media/bill/Drive/zotero/data/storage/SWQZ6TPE/Vanneschi et al. - 2014 - A survey of semantic methods in genetic programmin.pdf;/media/bill/Drive/zotero/data/storage/7K944VUJ/s10710-013-9210-0.html;/media/bill/Drive/zotero/data/storage/CC29FQQP/fulltext.html},
  journal = {Genetic Programming and Evolvable Machines},
  number = {2}
}

@article{vanschorenOpenMLNetworkedScience2013,
  title = {{{OpenML}}: {{Networked Science}} in {{Machine Learning}}},
  author = {Vanschoren, Joaquin and {van Rijn}, Jan N. and Bischl, Bernd and Torgo, Luis},
  year = {2013},
  volume = {15},
  pages = {49--60},
  doi = {10.1145/2641190.2641198},
  journal = {SIGKDD Explorations},
  number = {2}
}

@article{vanschorenOpenMLNetworkedScience2014a,
  title = {{{OpenML}}: {{Networked Science}} in {{Machine Learning}}},
  shorttitle = {{{OpenML}}},
  author = {Vanschoren, Joaquin and {van Rijn}, Jan N. and Bischl, Bernd and Torgo, Luis},
  year = {2014},
  month = jun,
  volume = {15},
  pages = {49--60},
  issn = {1931-0145},
  doi = {10.1145/2641190.2641198},
  abstract = {Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.},
  journal = {SIGKDD Explor. Newsl.},
  number = {2}
}

@inproceedings{virgolin2017scalable,
  title = {Scalable Genetic Programming by Gene-Pool Optimal Mixing and Input-Space Entropy-Based Building-Block Learning},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  author = {Virgolin, Marco and Alderliesten, Tanja and Witteveen, Cees and Bosman, Peter A N},
  year = {2017},
  pages = {1041--1048}
}

@article{virgolin2020improving,
  title = {Improving Model-Based Genetic Programming for Symbolic Regression of Small Expressions},
  author = {Virgolin, Marco and Alderliesten, Tanja and Witteveen, Cees and Bosman, Peter A N},
  year = {2020},
  pages = {tba},
  publisher = {{MIT Press}},
  journal = {Evolutionary Computation}
}

@inproceedings{virgolinLinearScalingSemantic2019,
  title = {Linear Scaling with and within Semantic Backpropagation-Based Genetic Programming for Symbolic Regression},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Virgolin, Marco and Alderliesten, Tanja and Bosman, Peter AN},
  year = {2019},
  pages = {1084--1092},
  file = {/media/bill/Drive/zotero/data/storage/TM96SWC4/Virgolin et al. - 2019 - Linear scaling with and within semantic backpropag.pdf;/media/bill/Drive/zotero/data/storage/KGFKWHMJ/3321707.html}
}

@article{vladislavlevaOrderNonlinearityComplexity2009a,
  title = {Order of {{Nonlinearity}} as a {{Complexity Measure}} for {{Models Generated}} by {{Symbolic Regression}} via {{Pareto Genetic Programming}}},
  author = {Vladislavleva, E.J. and Smits, G.F. and {den Hertog}, D.},
  year = {2009},
  volume = {13},
  pages = {333--349},
  issn = {1089-778X},
  doi = {10.1109/TEVC.2008.926486},
  abstract = {This paper presents a novel approach to generate data-driven regression models that not only give reliable prediction of the observed data but also have smoother response surfaces and extra generalization capabilities with respect to extrapolation. These models are obtained as solutions of a genetic programming (GP) process, where selection is guided by a tradeoff between two competing objectives - numerical accuracy and the order of nonlinearity. The latter is a novel complexity measure that adopts the notion of the minimal degree of the best-fit polynomial, approximating an analytical function with a certain precision. Using nine regression problems, this paper presents and illustrates two different strategies for the use of the order of nonlinearity in symbolic regression via GP. The combination of optimization of the order of nonlinearity together with the numerical accuracy strongly outperforms ldquoconventionalrdquo optimization of a size-related expressional complexity and the accuracy with respect to extrapolative capabilities of solutions on all nine test problems. In addition to exploiting the new complexity measure, this paper also introduces a novel heuristic of alternating several optimization objectives in a 2-D optimization framework. Alternating the objectives at each generation in such a way allows us to exploit the effectiveness of 2-D optimization when more than two objectives are of interest (in this paper, these are accuracy, expressional complexity, and the order of nonlinearity). Results of the experiments on all test problems suggest that alternating the order of nonlinearity of GP individuals with their structural complexity produces solutions that are both compact and have smoother response surfaces, and, hence, contributes to better interpretability and understanding.},
  journal = {IEEE Transactions on Evolutionary Computation},
  keywords = {best-fit polynomial,Complexity,computational complexity,data-driven regression models,evolutionary multiobjective optimization,extrapolation,Genetic algorithms,genetic programming (GP),industrial data analysis,model selection,nonlinearity order,Pareto genetic programming,regression analysis,Symbolic regression},
  number = {2}
}

@article{whiteBetterGPBenchmarks2012a,
  title = {Better {{GP}} Benchmarks: Community Survey Results and Proposals},
  shorttitle = {Better {{GP}} Benchmarks},
  author = {White, David R. and McDermott, James and Castelli, Mauro and Manzoni, Luca and Goldman, Brian W. and Kronberger, Gabriel and Ja{\'s}kowski, Wojciech and O'Reilly, Una-May and Luke, Sean},
  year = {2012},
  month = dec,
  volume = {14},
  pages = {3--29},
  issn = {1389-2576, 1573-7632},
  doi = {10.1007/s10710-012-9177-2},
  journal = {Genetic Programming and Evolvable Machines},
  number = {1}
}

@inproceedings{wieloch2013running,
  title = {Running Programs Backwards: Instruction Inversion for Effective Search in Semantic Spaces},
  booktitle = {Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation},
  author = {Wieloch, Bartosz and Krawiec, Krzysztof},
  year = {2013},
  pages = {1013--1020}
}

@article{zegklitzBenchmarkingStateoftheartSymbolic2020,
  title = {Benchmarking State-of-the-Art Symbolic Regression Algorithms},
  author = {{\v Z}egklitz, Jan and Po{\v s}{\'i}k, Petr},
  year = {2020},
  pages = {1--29},
  publisher = {{Springer}},
  file = {/media/bill/Drive/zotero/data/storage/5K6PI4WB/Žegklitz and Pošík - 2020 - Benchmarking state-of-the-art symbolic regression .pdf},
  journal = {Genetic Programming and Evolvable Machines}
}

@article{zegklitzSymbolicRegressionAlgorithms2017b,
  title = {Symbolic {{Regression Algorithms}} with {{Built}}-in {{Linear Regression}}},
  author = {{\v Z}egklitz, Jan and Po{\v s}{\'i}k, Petr},
  year = {2017},
  month = jan,
  abstract = {Recently, several algorithms for symbolic regression (SR) emerged which employ a form of multiple linear regression (LR) to produce generalized linear models. The use of LR allows the algorithms to create models with relatively small error right from the beginning of the search; such algorithms are thus claimed to be (sometimes by orders of magnitude) faster than SR algorithms based on vanilla genetic programming. However, a systematic comparison of these algorithms on a common set of problems is still missing. In this paper we conceptually and experimentally compare several representatives of such algorithms (GPTIPS, FFX, and EFS). They are applied as off-the-shelf, ready-to-use techniques, mostly using their default settings. The methods are compared on several synthetic and real-world SR benchmark problems. Their performance is also related to the performance of three conventional machine learning algorithms \textemdash{} multiple regression, random forests and support vector regression.},
  archiveprefix = {arXiv},
  eprint = {1701.03641},
  eprinttype = {arxiv},
  journal = {arXiv:1701.03641 [cs]},
  keywords = {Computer Science - Learning},
  primaryclass = {cs}
}


